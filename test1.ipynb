{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5944d2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-11T13:03:53.787974Z",
     "iopub.status.busy": "2022-02-11T13:03:53.786463Z",
     "iopub.status.idle": "2022-02-11T13:03:53.877781Z",
     "shell.execute_reply": "2022-02-11T13:03:53.875068Z",
     "shell.execute_reply.started": "2022-02-11T06:32:32.793811Z"
    },
    "papermill": {
     "duration": 0.099844,
     "end_time": "2022-02-11T13:03:53.877993",
     "exception": false,
     "start_time": "2022-02-11T13:03:53.778149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mitbih1/117.atr\n",
      "/kaggle/input/mitbih1/103.dat\n",
      "/kaggle/input/mitbih1/106.hea\n",
      "/kaggle/input/mitbih1/113.dat\n",
      "/kaggle/input/mitbih1/202.dat\n",
      "/kaggle/input/mitbih1/217.dat\n",
      "/kaggle/input/mitbih1/111.atr\n",
      "/kaggle/input/mitbih1/208.hea\n",
      "/kaggle/input/mitbih1/219.dat\n",
      "/kaggle/input/mitbih1/104.atr\n",
      "/kaggle/input/mitbih1/201.dat\n",
      "/kaggle/input/mitbih1/201.atr\n",
      "/kaggle/input/mitbih1/121.hea\n",
      "/kaggle/input/mitbih1/219.atr\n",
      "/kaggle/input/mitbih1/102.dat\n",
      "/kaggle/input/mitbih1/116.hea\n",
      "/kaggle/input/mitbih1/231.dat\n",
      "/kaggle/input/mitbih1/111.hea\n",
      "/kaggle/input/mitbih1/209.hea\n",
      "/kaggle/input/mitbih1/122.atr\n",
      "/kaggle/input/mitbih1/203.hea\n",
      "/kaggle/input/mitbih1/212.atr\n",
      "/kaggle/input/mitbih1/232.dat\n",
      "/kaggle/input/mitbih1/100.hea\n",
      "/kaggle/input/mitbih1/234.hea\n",
      "/kaggle/input/mitbih1/214.dat\n",
      "/kaggle/input/mitbih1/119.hea\n",
      "/kaggle/input/mitbih1/228.dat\n",
      "/kaggle/input/mitbih1/107.dat\n",
      "/kaggle/input/mitbih1/108.hea\n",
      "/kaggle/input/mitbih1/123.hea\n",
      "/kaggle/input/mitbih1/121.dat\n",
      "/kaggle/input/mitbih1/105.atr\n",
      "/kaggle/input/mitbih1/232.hea\n",
      "/kaggle/input/mitbih1/233.hea\n",
      "/kaggle/input/mitbih1/115.dat\n",
      "/kaggle/input/mitbih1/212.hea\n",
      "/kaggle/input/mitbih1/200.atr\n",
      "/kaggle/input/mitbih1/214.atr\n",
      "/kaggle/input/mitbih1/213.hea\n",
      "/kaggle/input/mitbih1/223.dat\n",
      "/kaggle/input/mitbih1/122.dat\n",
      "/kaggle/input/mitbih1/221.atr\n",
      "/kaggle/input/mitbih1/100.dat\n",
      "/kaggle/input/mitbih1/203.dat\n",
      "/kaggle/input/mitbih1/111.dat\n",
      "/kaggle/input/mitbih1/222.hea\n",
      "/kaggle/input/mitbih1/203.atr\n",
      "/kaggle/input/mitbih1/230.atr\n",
      "/kaggle/input/mitbih1/123.dat\n",
      "/kaggle/input/mitbih1/223.atr\n",
      "/kaggle/input/mitbih1/215.atr\n",
      "/kaggle/input/mitbih1/233.atr\n",
      "/kaggle/input/mitbih1/205.hea\n",
      "/kaggle/input/mitbih1/215.dat\n",
      "/kaggle/input/mitbih1/201.hea\n",
      "/kaggle/input/mitbih1/213.dat\n",
      "/kaggle/input/mitbih1/220.dat\n",
      "/kaggle/input/mitbih1/115.atr\n",
      "/kaggle/input/mitbih1/117.dat\n",
      "/kaggle/input/mitbih1/219.hea\n",
      "/kaggle/input/mitbih1/116.dat\n",
      "/kaggle/input/mitbih1/113.hea\n",
      "/kaggle/input/mitbih1/118.atr\n",
      "/kaggle/input/mitbih1/109.hea\n",
      "/kaggle/input/mitbih1/202.atr\n",
      "/kaggle/input/mitbih1/231.hea\n",
      "/kaggle/input/mitbih1/234.dat\n",
      "/kaggle/input/mitbih1/109.atr\n",
      "/kaggle/input/mitbih1/124.hea\n",
      "/kaggle/input/mitbih1/106.dat\n",
      "/kaggle/input/mitbih1/212.dat\n",
      "/kaggle/input/mitbih1/105.dat\n",
      "/kaggle/input/mitbih1/210.atr\n",
      "/kaggle/input/mitbih1/101.hea\n",
      "/kaggle/input/mitbih1/200.hea\n",
      "/kaggle/input/mitbih1/223.hea\n",
      "/kaggle/input/mitbih1/107.atr\n",
      "/kaggle/input/mitbih1/123.atr\n",
      "/kaggle/input/mitbih1/214.hea\n",
      "/kaggle/input/mitbih1/231.atr\n",
      "/kaggle/input/mitbih1/107.hea\n",
      "/kaggle/input/mitbih1/234.atr\n",
      "/kaggle/input/mitbih1/215.hea\n",
      "/kaggle/input/mitbih1/112.atr\n",
      "/kaggle/input/mitbih1/220.hea\n",
      "/kaggle/input/mitbih1/105.hea\n",
      "/kaggle/input/mitbih1/108.dat\n",
      "/kaggle/input/mitbih1/221.dat\n",
      "/kaggle/input/mitbih1/210.dat\n",
      "/kaggle/input/mitbih1/210.hea\n",
      "/kaggle/input/mitbih1/102.atr\n",
      "/kaggle/input/mitbih1/228.atr\n",
      "/kaggle/input/mitbih1/113.atr\n",
      "/kaggle/input/mitbih1/207.atr\n",
      "/kaggle/input/mitbih1/115.hea\n",
      "/kaggle/input/mitbih1/117.hea\n",
      "/kaggle/input/mitbih1/222.dat\n",
      "/kaggle/input/mitbih1/103.atr\n",
      "/kaggle/input/mitbih1/112.dat\n",
      "/kaggle/input/mitbih1/118.hea\n",
      "/kaggle/input/mitbih1/109.dat\n",
      "/kaggle/input/mitbih1/200.dat\n",
      "/kaggle/input/mitbih1/217.atr\n",
      "/kaggle/input/mitbih1/207.hea\n",
      "/kaggle/input/mitbih1/114.atr\n",
      "/kaggle/input/mitbih1/205.atr\n",
      "/kaggle/input/mitbih1/104.hea\n",
      "/kaggle/input/mitbih1/228.hea\n",
      "/kaggle/input/mitbih1/124.atr\n",
      "/kaggle/input/mitbih1/208.atr\n",
      "/kaggle/input/mitbih1/213.atr\n",
      "/kaggle/input/mitbih1/119.atr\n",
      "/kaggle/input/mitbih1/119.dat\n",
      "/kaggle/input/mitbih1/232.atr\n",
      "/kaggle/input/mitbih1/217.hea\n",
      "/kaggle/input/mitbih1/103.hea\n",
      "/kaggle/input/mitbih1/106.atr\n",
      "/kaggle/input/mitbih1/208.dat\n",
      "/kaggle/input/mitbih1/205.dat\n",
      "/kaggle/input/mitbih1/207.dat\n",
      "/kaggle/input/mitbih1/114.dat\n",
      "/kaggle/input/mitbih1/220.atr\n",
      "/kaggle/input/mitbih1/124.dat\n",
      "/kaggle/input/mitbih1/209.dat\n",
      "/kaggle/input/mitbih1/114.hea\n",
      "/kaggle/input/mitbih1/100.atr\n",
      "/kaggle/input/mitbih1/108.atr\n",
      "/kaggle/input/mitbih1/230.dat\n",
      "/kaggle/input/mitbih1/121.atr\n",
      "/kaggle/input/mitbih1/221.hea\n",
      "/kaggle/input/mitbih1/122.hea\n",
      "/kaggle/input/mitbih1/101.atr\n",
      "/kaggle/input/mitbih1/233.dat\n",
      "/kaggle/input/mitbih1/file_list.txt\n",
      "/kaggle/input/mitbih1/104.dat\n",
      "/kaggle/input/mitbih1/102.hea\n",
      "/kaggle/input/mitbih1/222.atr\n",
      "/kaggle/input/mitbih1/209.atr\n",
      "/kaggle/input/mitbih1/118.dat\n",
      "/kaggle/input/mitbih1/202.hea\n",
      "/kaggle/input/mitbih1/112.hea\n",
      "/kaggle/input/mitbih1/101.dat\n",
      "/kaggle/input/mitbih1/116.atr\n",
      "/kaggle/input/mitbih1/230.hea\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e9ec51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T13:03:53.899196Z",
     "iopub.status.busy": "2022-02-11T13:03:53.898357Z",
     "iopub.status.idle": "2022-02-11T13:04:02.470099Z",
     "shell.execute_reply": "2022-02-11T13:04:02.469585Z",
     "shell.execute_reply.started": "2022-02-11T06:32:32.891153Z"
    },
    "papermill": {
     "duration": 8.584296,
     "end_time": "2022-02-11T13:04:02.470250",
     "exception": false,
     "start_time": "2022-02-11T13:03:53.885954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample\n",
    "import glob\n",
    "import pywt\n",
    "\n",
    "def WTfilt_1d(sig):\n",
    "    \"\"\"\n",
    "    对信号进行小波变换滤波\n",
    "    :param sig: 输入信号，1-d array\n",
    "    :return: 小波滤波后的信号，1-d array\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt    \n",
    "\n",
    "\n",
    "# -------------------------心拍截取-------------------\n",
    "def heartbeat(file0):\n",
    "    '''\n",
    "    file0:下载的MITAB数据\n",
    "    \n",
    "    '''\n",
    "    N_Seg = []; SVEB_Seg = [];  VEB_Seg = []; F_Seg = [] ; Q_Seg = [];\n",
    "    file = list(set(file0))\n",
    "    \n",
    "    for f in range(len(file)):\n",
    "        annotation = wfdb.rdann(panth+file[f][-7:-4],'atr')\n",
    "        record_name = annotation.record_name    # 读取记录名称\n",
    "        Record = wfdb.rdsamp(panth+record_name)[0][:, 0]  # 一般只取一个导联\n",
    "        record = WTfilt_1d(Record)         # 小波去噪\n",
    "        label = annotation.symbol  # 心拍标签列表\n",
    "        label_index = annotation.sample   # 标签索引列表\n",
    "        for j in range(len(label_index)):\n",
    "            if label_index[j] >= 144 and (label_index[j]+180) <= 650000:\n",
    "                if label[j] == 'N' or label[j] == '.' or label[j] == 'L' or label[j] == 'R' or label[j] == 'e' or label[j] == 'j':\n",
    "                    Seg = record[label_index[j]-144:label_index[j]+180]  # R峰的前0.4s和后0.5s\n",
    "                    N_Seg.append(Seg)\n",
    "                    \n",
    "                if label[j] == 'A' or label[j] == 'a' or label[j] == 'J' or label[j] == 'S':\n",
    "                    \n",
    "                    Seg = record[label_index[j]-144:label_index[j]+180]\n",
    "                    SVEB_Seg.append(Seg)\n",
    "                    \n",
    "                if label[j] == 'V' or label[j] == 'E':\n",
    "                   \n",
    "                    Seg = record[label_index[j]-144:label_index[j]+180]\n",
    "                    VEB_Seg.append(Seg)\n",
    "                    \n",
    "                if label[j] == 'F':\n",
    "                    \n",
    "                    Seg = record[label_index[j]-144:label_index[j]+180]\n",
    "                    F_Seg.append(Seg)\n",
    "\n",
    "                if label[j] == '/' or label[j] == 'f' or label[j] == 'Q':\n",
    "                    \n",
    "                    Seg = record[label_index[j]-144:label_index[j]+180]\n",
    "                    Q_Seg.append(Seg)\n",
    "                    \n",
    "    N_segement = np.array(N_Seg)\n",
    "    SVEB_segement = np.array(SVEB_Seg)\n",
    "    VEB_segement = np.array(VEB_Seg)\n",
    "    F_segement = np.array(F_Seg)\n",
    "    Q_segement = np.array(Q_Seg)\n",
    "    \n",
    "    label_N = np.zeros(N_segement.shape[0])\n",
    "    label_SVEB = np.ones(SVEB_segement.shape[0])\n",
    "    label_VEB = np.ones(VEB_segement.shape[0])*2\n",
    "    label_F = np.ones(F_segement.shape[0])*3\n",
    "    label_Q = np.ones(Q_segement.shape[0])*4\n",
    "                    \n",
    "    Data = np.concatenate((N_segement, SVEB_segement, VEB_segement, F_segement, Q_segement), axis=0)\n",
    "    Label = np.concatenate((label_N, label_SVEB, label_VEB, label_F, label_Q), axis=0)\n",
    "    \n",
    "    return Data, Label\n",
    "\n",
    "#-----------------------心拍截取和保存---------------------\n",
    "#建议一次性截取和保存，不需要重复操作，下次训练和测试的时候，直接load\n",
    "panth='/kaggle/input/mitbih1/'\n",
    "file = glob.glob(panth+'*.hea')\n",
    "Data, Label=heartbeat(file)\n",
    "\n",
    "Data=np.save('/kaggle/working/'+'Data',Data)\n",
    "Label=np.save('/kaggle/working/'+'Label',Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00871f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T13:04:02.498554Z",
     "iopub.status.busy": "2022-02-11T13:04:02.497797Z",
     "iopub.status.idle": "2022-02-11T15:45:22.718745Z",
     "shell.execute_reply": "2022-02-11T15:45:22.719163Z",
     "shell.execute_reply.started": "2022-02-11T08:39:44.030087Z"
    },
    "papermill": {
     "duration": 9680.244772,
     "end_time": "2022-02-11T15:45:22.719336",
     "exception": false,
     "start_time": "2022-02-11T13:04:02.474564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 5 test: 0  train: other\n",
      "Model: \"mylstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 324, 64)           75008     \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 5)                 2654981   \n",
      "=================================================================\n",
      "Total params: 2,729,989\n",
      "Trainable params: 2,729,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "685/685 [==============================] - 60s 75ms/step - loss: 0.1392 - categorical_accuracy: 0.9621 - val_loss: 0.0582 - val_categorical_accuracy: 0.9843\n",
      "Epoch 2/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0618 - categorical_accuracy: 0.9823 - val_loss: 0.0431 - val_categorical_accuracy: 0.9881\n",
      "Epoch 3/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0473 - categorical_accuracy: 0.9860 - val_loss: 0.0414 - val_categorical_accuracy: 0.9883\n",
      "Epoch 4/30\n",
      "685/685 [==============================] - 52s 73ms/step - loss: 0.0409 - categorical_accuracy: 0.9878 - val_loss: 0.0397 - val_categorical_accuracy: 0.9894\n",
      "Epoch 5/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0352 - categorical_accuracy: 0.9891 - val_loss: 0.0368 - val_categorical_accuracy: 0.9900\n",
      "Epoch 6/30\n",
      "685/685 [==============================] - 54s 73ms/step - loss: 0.0366 - categorical_accuracy: 0.9885 - val_loss: 0.0351 - val_categorical_accuracy: 0.9905\n",
      "Epoch 7/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0278 - categorical_accuracy: 0.9911 - val_loss: 0.0391 - val_categorical_accuracy: 0.9906\n",
      "Epoch 8/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0256 - categorical_accuracy: 0.9920 - val_loss: 0.0320 - val_categorical_accuracy: 0.9921\n",
      "Epoch 9/30\n",
      "685/685 [==============================] - 54s 73ms/step - loss: 0.0246 - categorical_accuracy: 0.9921 - val_loss: 0.0367 - val_categorical_accuracy: 0.9904\n",
      "Epoch 10/30\n",
      "685/685 [==============================] - 55s 73ms/step - loss: 0.0230 - categorical_accuracy: 0.9924 - val_loss: 0.0390 - val_categorical_accuracy: 0.9917\n",
      "Epoch 11/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0200 - categorical_accuracy: 0.9936 - val_loss: 0.0351 - val_categorical_accuracy: 0.9915\n",
      "Epoch 12/30\n",
      "685/685 [==============================] - 54s 72ms/step - loss: 0.0194 - categorical_accuracy: 0.9936 - val_loss: 0.0367 - val_categorical_accuracy: 0.9911\n",
      "Epoch 13/30\n",
      "685/685 [==============================] - 55s 73ms/step - loss: 0.0197 - categorical_accuracy: 0.9934 - val_loss: 0.0386 - val_categorical_accuracy: 0.9912\n",
      "Epoch 14/30\n",
      "685/685 [==============================] - 54s 72ms/step - loss: 0.0169 - categorical_accuracy: 0.9943 - val_loss: 0.0387 - val_categorical_accuracy: 0.9915\n",
      "Epoch 15/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0172 - categorical_accuracy: 0.9943 - val_loss: 0.0353 - val_categorical_accuracy: 0.9916\n",
      "Epoch 16/30\n",
      "685/685 [==============================] - 54s 72ms/step - loss: 0.0145 - categorical_accuracy: 0.9950 - val_loss: 0.0457 - val_categorical_accuracy: 0.9907\n",
      "Epoch 17/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0148 - categorical_accuracy: 0.9949 - val_loss: 0.0437 - val_categorical_accuracy: 0.9915\n",
      "Epoch 18/30\n",
      "685/685 [==============================] - 54s 71ms/step - loss: 0.0138 - categorical_accuracy: 0.9954 - val_loss: 0.0404 - val_categorical_accuracy: 0.9919\n",
      "Epoch 19/30\n",
      "685/685 [==============================] - 53s 71ms/step - loss: 0.0135 - categorical_accuracy: 0.9951 - val_loss: 0.0441 - val_categorical_accuracy: 0.9919\n",
      "Epoch 20/30\n",
      "685/685 [==============================] - 50s 70ms/step - loss: 0.0133 - categorical_accuracy: 0.9955 - val_loss: 0.0416 - val_categorical_accuracy: 0.9921\n",
      "Epoch 21/30\n",
      "685/685 [==============================] - 53s 70ms/step - loss: 0.0132 - categorical_accuracy: 0.9956 - val_loss: 0.0409 - val_categorical_accuracy: 0.9918\n",
      "Epoch 22/30\n",
      "685/685 [==============================] - 52s 69ms/step - loss: 0.0123 - categorical_accuracy: 0.9958 - val_loss: 0.0404 - val_categorical_accuracy: 0.9921\n",
      "Epoch 23/30\n",
      "685/685 [==============================] - 50s 70ms/step - loss: 0.0126 - categorical_accuracy: 0.9955 - val_loss: 0.0421 - val_categorical_accuracy: 0.9919\n",
      "Epoch 24/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0109 - categorical_accuracy: 0.9960 - val_loss: 0.0447 - val_categorical_accuracy: 0.9920\n",
      "Epoch 25/30\n",
      "685/685 [==============================] - 55s 73ms/step - loss: 0.0108 - categorical_accuracy: 0.9965 - val_loss: 0.0493 - val_categorical_accuracy: 0.9922\n",
      "Epoch 26/30\n",
      "685/685 [==============================] - 52s 73ms/step - loss: 0.0100 - categorical_accuracy: 0.9962 - val_loss: 0.0474 - val_categorical_accuracy: 0.9925\n",
      "Epoch 27/30\n",
      "685/685 [==============================] - 52s 73ms/step - loss: 0.0103 - categorical_accuracy: 0.9965 - val_loss: 0.0474 - val_categorical_accuracy: 0.9923\n",
      "Epoch 28/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0104 - categorical_accuracy: 0.9964 - val_loss: 0.0483 - val_categorical_accuracy: 0.9914\n",
      "Epoch 29/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0101 - categorical_accuracy: 0.9965 - val_loss: 0.0508 - val_categorical_accuracy: 0.9921\n",
      "Epoch 30/30\n",
      "685/685 [==============================] - 54s 72ms/step - loss: 0.0095 - categorical_accuracy: 0.9966 - val_loss: 0.0490 - val_categorical_accuracy: 0.9921\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.0490 - categorical_accuracy: 0.9921\n",
      "fold: 5 test: 1  train: other\n",
      "Model: \"mylstm_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 324, 64)           75008     \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 5)                 2654981   \n",
      "=================================================================\n",
      "Total params: 2,729,989\n",
      "Trainable params: 2,729,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "685/685 [==============================] - 58s 73ms/step - loss: 0.1395 - categorical_accuracy: 0.9612 - val_loss: 0.0633 - val_categorical_accuracy: 0.9822\n",
      "Epoch 2/30\n",
      "685/685 [==============================] - 54s 72ms/step - loss: 0.0625 - categorical_accuracy: 0.9825 - val_loss: 0.0462 - val_categorical_accuracy: 0.9871\n",
      "Epoch 3/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0476 - categorical_accuracy: 0.9865 - val_loss: 0.0486 - val_categorical_accuracy: 0.9875\n",
      "Epoch 4/30\n",
      "685/685 [==============================] - 52s 71ms/step - loss: 0.0411 - categorical_accuracy: 0.9875 - val_loss: 0.0433 - val_categorical_accuracy: 0.9889\n",
      "Epoch 5/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0352 - categorical_accuracy: 0.9892 - val_loss: 0.0432 - val_categorical_accuracy: 0.9884\n",
      "Epoch 6/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0316 - categorical_accuracy: 0.9905 - val_loss: 0.0370 - val_categorical_accuracy: 0.9896\n",
      "Epoch 7/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0275 - categorical_accuracy: 0.9912 - val_loss: 0.0419 - val_categorical_accuracy: 0.9898\n",
      "Epoch 8/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0254 - categorical_accuracy: 0.9925 - val_loss: 0.0435 - val_categorical_accuracy: 0.9898\n",
      "Epoch 9/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0239 - categorical_accuracy: 0.9920 - val_loss: 0.0429 - val_categorical_accuracy: 0.9896\n",
      "Epoch 10/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0223 - categorical_accuracy: 0.9930 - val_loss: 0.0394 - val_categorical_accuracy: 0.9901\n",
      "Epoch 11/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0198 - categorical_accuracy: 0.9938 - val_loss: 0.0419 - val_categorical_accuracy: 0.9914\n",
      "Epoch 12/30\n",
      "685/685 [==============================] - 54s 73ms/step - loss: 0.0185 - categorical_accuracy: 0.9940 - val_loss: 0.0492 - val_categorical_accuracy: 0.9899\n",
      "Epoch 13/30\n",
      "685/685 [==============================] - 50s 71ms/step - loss: 0.0190 - categorical_accuracy: 0.9936 - val_loss: 0.0500 - val_categorical_accuracy: 0.9907\n",
      "Epoch 14/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0175 - categorical_accuracy: 0.9941 - val_loss: 0.0481 - val_categorical_accuracy: 0.9910\n",
      "Epoch 15/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0166 - categorical_accuracy: 0.9946 - val_loss: 0.0469 - val_categorical_accuracy: 0.9905\n",
      "Epoch 16/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0148 - categorical_accuracy: 0.9953 - val_loss: 0.0470 - val_categorical_accuracy: 0.9908\n",
      "Epoch 17/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0148 - categorical_accuracy: 0.9947 - val_loss: 0.0520 - val_categorical_accuracy: 0.9909\n",
      "Epoch 18/30\n",
      "685/685 [==============================] - 55s 73ms/step - loss: 0.0139 - categorical_accuracy: 0.9954 - val_loss: 0.0555 - val_categorical_accuracy: 0.9909\n",
      "Epoch 19/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0133 - categorical_accuracy: 0.9957 - val_loss: 0.0475 - val_categorical_accuracy: 0.9910\n",
      "Epoch 20/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0124 - categorical_accuracy: 0.9960 - val_loss: 0.0569 - val_categorical_accuracy: 0.9913\n",
      "Epoch 21/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0125 - categorical_accuracy: 0.9956 - val_loss: 0.0663 - val_categorical_accuracy: 0.9902\n",
      "Epoch 22/30\n",
      "685/685 [==============================] - 50s 69ms/step - loss: 0.0134 - categorical_accuracy: 0.9957 - val_loss: 0.0551 - val_categorical_accuracy: 0.9912\n",
      "Epoch 23/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0108 - categorical_accuracy: 0.9962 - val_loss: 0.0562 - val_categorical_accuracy: 0.9906\n",
      "Epoch 24/30\n",
      "685/685 [==============================] - 50s 70ms/step - loss: 0.0110 - categorical_accuracy: 0.9963 - val_loss: 0.0566 - val_categorical_accuracy: 0.9913\n",
      "Epoch 25/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0109 - categorical_accuracy: 0.9964 - val_loss: 0.0647 - val_categorical_accuracy: 0.9904\n",
      "Epoch 26/30\n",
      "685/685 [==============================] - 52s 73ms/step - loss: 0.0096 - categorical_accuracy: 0.9967 - val_loss: 0.0543 - val_categorical_accuracy: 0.9915\n",
      "Epoch 27/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0096 - categorical_accuracy: 0.9969 - val_loss: 0.0615 - val_categorical_accuracy: 0.9911\n",
      "Epoch 28/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0106 - categorical_accuracy: 0.9965 - val_loss: 0.0632 - val_categorical_accuracy: 0.9897\n",
      "Epoch 29/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0098 - categorical_accuracy: 0.9967 - val_loss: 0.0586 - val_categorical_accuracy: 0.9907\n",
      "Epoch 30/30\n",
      "685/685 [==============================] - 53s 73ms/step - loss: 0.0087 - categorical_accuracy: 0.9972 - val_loss: 0.0583 - val_categorical_accuracy: 0.9921\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.0583 - categorical_accuracy: 0.9921\n",
      "fold: 5 test: 2  train: other\n",
      "Model: \"mylstm_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_4 (Sequential)    (None, 324, 64)           75008     \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    (None, 5)                 2654981   \n",
      "=================================================================\n",
      "Total params: 2,729,989\n",
      "Trainable params: 2,729,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "685/685 [==============================] - 58s 73ms/step - loss: 0.1373 - categorical_accuracy: 0.9624 - val_loss: 0.0752 - val_categorical_accuracy: 0.9797\n",
      "Epoch 2/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0644 - categorical_accuracy: 0.9825 - val_loss: 0.0563 - val_categorical_accuracy: 0.9845\n",
      "Epoch 3/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0507 - categorical_accuracy: 0.9861 - val_loss: 0.0483 - val_categorical_accuracy: 0.9861\n",
      "Epoch 4/30\n",
      "685/685 [==============================] - 52s 74ms/step - loss: 0.0413 - categorical_accuracy: 0.9884 - val_loss: 0.0451 - val_categorical_accuracy: 0.9879\n",
      "Epoch 5/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0365 - categorical_accuracy: 0.9896 - val_loss: 0.0407 - val_categorical_accuracy: 0.9885\n",
      "Epoch 6/30\n",
      "685/685 [==============================] - 55s 72ms/step - loss: 0.0325 - categorical_accuracy: 0.9901 - val_loss: 0.0461 - val_categorical_accuracy: 0.9882\n",
      "Epoch 7/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0308 - categorical_accuracy: 0.9907 - val_loss: 0.0384 - val_categorical_accuracy: 0.9894\n",
      "Epoch 8/30\n",
      "685/685 [==============================] - 54s 72ms/step - loss: 0.0264 - categorical_accuracy: 0.9918 - val_loss: 0.0405 - val_categorical_accuracy: 0.9893\n",
      "Epoch 9/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0253 - categorical_accuracy: 0.9924 - val_loss: 0.0429 - val_categorical_accuracy: 0.9902\n",
      "Epoch 10/30\n",
      "685/685 [==============================] - 50s 69ms/step - loss: 0.0219 - categorical_accuracy: 0.9933 - val_loss: 0.0389 - val_categorical_accuracy: 0.9901\n",
      "Epoch 11/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0200 - categorical_accuracy: 0.9936 - val_loss: 0.0393 - val_categorical_accuracy: 0.9897\n",
      "Epoch 12/30\n",
      "685/685 [==============================] - 51s 70ms/step - loss: 0.0191 - categorical_accuracy: 0.9941 - val_loss: 0.0421 - val_categorical_accuracy: 0.9904\n",
      "Epoch 13/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0189 - categorical_accuracy: 0.9939 - val_loss: 0.0471 - val_categorical_accuracy: 0.9902\n",
      "Epoch 14/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0166 - categorical_accuracy: 0.9946 - val_loss: 0.0435 - val_categorical_accuracy: 0.9906\n",
      "Epoch 15/30\n",
      "685/685 [==============================] - 55s 74ms/step - loss: 0.0158 - categorical_accuracy: 0.9951 - val_loss: 0.0452 - val_categorical_accuracy: 0.9905\n",
      "Epoch 16/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0151 - categorical_accuracy: 0.9949 - val_loss: 0.0466 - val_categorical_accuracy: 0.9903\n",
      "Epoch 17/30\n",
      "685/685 [==============================] - 50s 70ms/step - loss: 0.0192 - categorical_accuracy: 0.9936 - val_loss: 0.0485 - val_categorical_accuracy: 0.9903\n",
      "Epoch 18/30\n",
      "685/685 [==============================] - 53s 74ms/step - loss: 0.0134 - categorical_accuracy: 0.9956 - val_loss: 0.0472 - val_categorical_accuracy: 0.9911\n",
      "Epoch 19/30\n",
      "685/685 [==============================] - 52s 73ms/step - loss: 0.0127 - categorical_accuracy: 0.9960 - val_loss: 0.0474 - val_categorical_accuracy: 0.9904\n",
      "Epoch 20/30\n",
      "685/685 [==============================] - 52s 69ms/step - loss: 0.0115 - categorical_accuracy: 0.9960 - val_loss: 0.0470 - val_categorical_accuracy: 0.9912\n",
      "Epoch 21/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0113 - categorical_accuracy: 0.9960 - val_loss: 0.0435 - val_categorical_accuracy: 0.9914\n",
      "Epoch 22/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0113 - categorical_accuracy: 0.9963 - val_loss: 0.0579 - val_categorical_accuracy: 0.9902\n",
      "Epoch 23/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0113 - categorical_accuracy: 0.9960 - val_loss: 0.0536 - val_categorical_accuracy: 0.9914\n",
      "Epoch 24/30\n",
      "685/685 [==============================] - 49s 69ms/step - loss: 0.0122 - categorical_accuracy: 0.9960 - val_loss: 0.0583 - val_categorical_accuracy: 0.9906\n",
      "Epoch 25/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0102 - categorical_accuracy: 0.9963 - val_loss: 0.0559 - val_categorical_accuracy: 0.9910\n",
      "Epoch 26/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0103 - categorical_accuracy: 0.9965 - val_loss: 0.0571 - val_categorical_accuracy: 0.9912\n",
      "Epoch 27/30\n",
      "685/685 [==============================] - 49s 69ms/step - loss: 0.0102 - categorical_accuracy: 0.9967 - val_loss: 0.0562 - val_categorical_accuracy: 0.9912\n",
      "Epoch 28/30\n",
      "685/685 [==============================] - 52s 72ms/step - loss: 0.0092 - categorical_accuracy: 0.9970 - val_loss: 0.0516 - val_categorical_accuracy: 0.9908\n",
      "Epoch 29/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0088 - categorical_accuracy: 0.9971 - val_loss: 0.0546 - val_categorical_accuracy: 0.9908\n",
      "Epoch 30/30\n",
      "685/685 [==============================] - 49s 68ms/step - loss: 0.0091 - categorical_accuracy: 0.9968 - val_loss: 0.0547 - val_categorical_accuracy: 0.9913\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.0547 - categorical_accuracy: 0.9913\n",
      "fold: 5 test: 3  train: other\n",
      "Model: \"mylstm_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_6 (Sequential)    (None, 324, 64)           75008     \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 5)                 2654981   \n",
      "=================================================================\n",
      "Total params: 2,729,989\n",
      "Trainable params: 2,729,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "685/685 [==============================] - 57s 72ms/step - loss: 0.1434 - categorical_accuracy: 0.9608 - val_loss: 0.0690 - val_categorical_accuracy: 0.9813\n",
      "Epoch 2/30\n",
      "685/685 [==============================] - 49s 68ms/step - loss: 0.0649 - categorical_accuracy: 0.9819 - val_loss: 0.0484 - val_categorical_accuracy: 0.9868\n",
      "Epoch 3/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0488 - categorical_accuracy: 0.9861 - val_loss: 0.0433 - val_categorical_accuracy: 0.9877\n",
      "Epoch 4/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0403 - categorical_accuracy: 0.9878 - val_loss: 0.0497 - val_categorical_accuracy: 0.9864\n",
      "Epoch 5/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0426 - categorical_accuracy: 0.9872 - val_loss: 0.0418 - val_categorical_accuracy: 0.9894\n",
      "Epoch 6/30\n",
      "685/685 [==============================] - 49s 68ms/step - loss: 0.0330 - categorical_accuracy: 0.9901 - val_loss: 0.0413 - val_categorical_accuracy: 0.9894\n",
      "Epoch 7/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0295 - categorical_accuracy: 0.9908 - val_loss: 0.0384 - val_categorical_accuracy: 0.9903\n",
      "Epoch 8/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0282 - categorical_accuracy: 0.9914 - val_loss: 0.0376 - val_categorical_accuracy: 0.9901\n",
      "Epoch 9/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0252 - categorical_accuracy: 0.9919 - val_loss: 0.0412 - val_categorical_accuracy: 0.9897\n",
      "Epoch 10/30\n",
      "685/685 [==============================] - 48s 67ms/step - loss: 0.0225 - categorical_accuracy: 0.9929 - val_loss: 0.0453 - val_categorical_accuracy: 0.9902\n",
      "Epoch 11/30\n",
      "685/685 [==============================] - 53s 70ms/step - loss: 0.0225 - categorical_accuracy: 0.9932 - val_loss: 0.0425 - val_categorical_accuracy: 0.9905\n",
      "Epoch 12/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0217 - categorical_accuracy: 0.9934 - val_loss: 0.0386 - val_categorical_accuracy: 0.9914\n",
      "Epoch 13/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0193 - categorical_accuracy: 0.9934 - val_loss: 0.0420 - val_categorical_accuracy: 0.9907\n",
      "Epoch 14/30\n",
      "685/685 [==============================] - 50s 70ms/step - loss: 0.0180 - categorical_accuracy: 0.9937 - val_loss: 0.0373 - val_categorical_accuracy: 0.9916\n",
      "Epoch 15/30\n",
      "685/685 [==============================] - 51s 68ms/step - loss: 0.0184 - categorical_accuracy: 0.9938 - val_loss: 0.0404 - val_categorical_accuracy: 0.9912\n",
      "Epoch 16/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0156 - categorical_accuracy: 0.9946 - val_loss: 0.0413 - val_categorical_accuracy: 0.9914\n",
      "Epoch 17/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0175 - categorical_accuracy: 0.9943 - val_loss: 0.0412 - val_categorical_accuracy: 0.9915\n",
      "Epoch 18/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0154 - categorical_accuracy: 0.9949 - val_loss: 0.0433 - val_categorical_accuracy: 0.9915\n",
      "Epoch 19/30\n",
      "685/685 [==============================] - 48s 67ms/step - loss: 0.0139 - categorical_accuracy: 0.9953 - val_loss: 0.0413 - val_categorical_accuracy: 0.9919\n",
      "Epoch 20/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0138 - categorical_accuracy: 0.9954 - val_loss: 0.0481 - val_categorical_accuracy: 0.9911\n",
      "Epoch 21/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0144 - categorical_accuracy: 0.9953 - val_loss: 0.0439 - val_categorical_accuracy: 0.9913\n",
      "Epoch 22/30\n",
      "685/685 [==============================] - 51s 70ms/step - loss: 0.0132 - categorical_accuracy: 0.9955 - val_loss: 0.0448 - val_categorical_accuracy: 0.9917\n",
      "Epoch 23/30\n",
      "685/685 [==============================] - 52s 68ms/step - loss: 0.0129 - categorical_accuracy: 0.9956 - val_loss: 0.0452 - val_categorical_accuracy: 0.9924\n",
      "Epoch 24/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0123 - categorical_accuracy: 0.9959 - val_loss: 0.0482 - val_categorical_accuracy: 0.9907\n",
      "Epoch 25/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0121 - categorical_accuracy: 0.9959 - val_loss: 0.0446 - val_categorical_accuracy: 0.9919\n",
      "Epoch 26/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0108 - categorical_accuracy: 0.9965 - val_loss: 0.0478 - val_categorical_accuracy: 0.9920\n",
      "Epoch 27/30\n",
      "685/685 [==============================] - 50s 67ms/step - loss: 0.0117 - categorical_accuracy: 0.9961 - val_loss: 0.0451 - val_categorical_accuracy: 0.9924\n",
      "Epoch 28/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0119 - categorical_accuracy: 0.9960 - val_loss: 0.0465 - val_categorical_accuracy: 0.9928\n",
      "Epoch 29/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0105 - categorical_accuracy: 0.9965 - val_loss: 0.0429 - val_categorical_accuracy: 0.9918\n",
      "Epoch 30/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0100 - categorical_accuracy: 0.9967 - val_loss: 0.0554 - val_categorical_accuracy: 0.9912\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.0554 - categorical_accuracy: 0.9912\n",
      "fold: 5 test: 4  train: other\n",
      "Model: \"mylstm_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_8 (Sequential)    (None, 324, 64)           75008     \n",
      "_________________________________________________________________\n",
      "sequential_9 (Sequential)    (None, 5)                 2654981   \n",
      "=================================================================\n",
      "Total params: 2,729,989\n",
      "Trainable params: 2,729,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "685/685 [==============================] - 57s 72ms/step - loss: 0.1367 - categorical_accuracy: 0.9624 - val_loss: 0.0642 - val_categorical_accuracy: 0.9820\n",
      "Epoch 2/30\n",
      "685/685 [==============================] - 49s 68ms/step - loss: 0.0630 - categorical_accuracy: 0.9827 - val_loss: 0.0533 - val_categorical_accuracy: 0.9848\n",
      "Epoch 3/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0474 - categorical_accuracy: 0.9868 - val_loss: 0.0444 - val_categorical_accuracy: 0.9882\n",
      "Epoch 4/30\n",
      "685/685 [==============================] - 51s 70ms/step - loss: 0.0402 - categorical_accuracy: 0.9883 - val_loss: 0.0428 - val_categorical_accuracy: 0.9877\n",
      "Epoch 5/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0348 - categorical_accuracy: 0.9896 - val_loss: 0.0411 - val_categorical_accuracy: 0.9889\n",
      "Epoch 6/30\n",
      "685/685 [==============================] - 50s 66ms/step - loss: 0.0308 - categorical_accuracy: 0.9905 - val_loss: 0.0401 - val_categorical_accuracy: 0.9892\n",
      "Epoch 7/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0278 - categorical_accuracy: 0.9910 - val_loss: 0.0429 - val_categorical_accuracy: 0.9907\n",
      "Epoch 8/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0252 - categorical_accuracy: 0.9919 - val_loss: 0.0402 - val_categorical_accuracy: 0.9904\n",
      "Epoch 9/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0226 - categorical_accuracy: 0.9927 - val_loss: 0.0441 - val_categorical_accuracy: 0.9899\n",
      "Epoch 10/30\n",
      "685/685 [==============================] - 50s 66ms/step - loss: 0.0215 - categorical_accuracy: 0.9931 - val_loss: 0.0463 - val_categorical_accuracy: 0.9892\n",
      "Epoch 11/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0209 - categorical_accuracy: 0.9933 - val_loss: 0.0446 - val_categorical_accuracy: 0.9902\n",
      "Epoch 12/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0192 - categorical_accuracy: 0.9940 - val_loss: 0.0509 - val_categorical_accuracy: 0.9898\n",
      "Epoch 13/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0173 - categorical_accuracy: 0.9943 - val_loss: 0.0420 - val_categorical_accuracy: 0.9900\n",
      "Epoch 14/30\n",
      "685/685 [==============================] - 48s 67ms/step - loss: 0.0172 - categorical_accuracy: 0.9943 - val_loss: 0.0477 - val_categorical_accuracy: 0.9903\n",
      "Epoch 15/30\n",
      "685/685 [==============================] - 53s 70ms/step - loss: 0.0150 - categorical_accuracy: 0.9949 - val_loss: 0.0463 - val_categorical_accuracy: 0.9899\n",
      "Epoch 16/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0155 - categorical_accuracy: 0.9951 - val_loss: 0.0501 - val_categorical_accuracy: 0.9901\n",
      "Epoch 17/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0143 - categorical_accuracy: 0.9950 - val_loss: 0.0484 - val_categorical_accuracy: 0.9898\n",
      "Epoch 18/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0137 - categorical_accuracy: 0.9953 - val_loss: 0.0442 - val_categorical_accuracy: 0.9901\n",
      "Epoch 19/30\n",
      "685/685 [==============================] - 49s 68ms/step - loss: 0.0128 - categorical_accuracy: 0.9956 - val_loss: 0.0586 - val_categorical_accuracy: 0.9903\n",
      "Epoch 20/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0124 - categorical_accuracy: 0.9959 - val_loss: 0.0510 - val_categorical_accuracy: 0.9908\n",
      "Epoch 21/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0122 - categorical_accuracy: 0.9959 - val_loss: 0.0625 - val_categorical_accuracy: 0.9894\n",
      "Epoch 22/30\n",
      "685/685 [==============================] - 52s 71ms/step - loss: 0.0121 - categorical_accuracy: 0.9959 - val_loss: 0.0569 - val_categorical_accuracy: 0.9902\n",
      "Epoch 23/30\n",
      "685/685 [==============================] - 50s 66ms/step - loss: 0.0115 - categorical_accuracy: 0.9960 - val_loss: 0.0534 - val_categorical_accuracy: 0.9902\n",
      "Epoch 24/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0120 - categorical_accuracy: 0.9960 - val_loss: 0.0501 - val_categorical_accuracy: 0.9910\n",
      "Epoch 25/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0094 - categorical_accuracy: 0.9968 - val_loss: 0.0563 - val_categorical_accuracy: 0.9910\n",
      "Epoch 26/30\n",
      "685/685 [==============================] - 51s 72ms/step - loss: 0.0111 - categorical_accuracy: 0.9963 - val_loss: 0.0526 - val_categorical_accuracy: 0.9903\n",
      "Epoch 27/30\n",
      "685/685 [==============================] - 50s 69ms/step - loss: 0.0091 - categorical_accuracy: 0.9966 - val_loss: 0.0580 - val_categorical_accuracy: 0.9905\n",
      "Epoch 28/30\n",
      "685/685 [==============================] - 52s 70ms/step - loss: 0.0093 - categorical_accuracy: 0.9968 - val_loss: 0.0593 - val_categorical_accuracy: 0.9911\n",
      "Epoch 29/30\n",
      "685/685 [==============================] - 52s 71ms/step - loss: 0.0100 - categorical_accuracy: 0.9965 - val_loss: 0.0588 - val_categorical_accuracy: 0.9907\n",
      "Epoch 30/30\n",
      "685/685 [==============================] - 51s 71ms/step - loss: 0.0080 - categorical_accuracy: 0.9970 - val_loss: 0.0619 - val_categorical_accuracy: 0.9912\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.0619 - categorical_accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, MaxPooling1D, Conv1D, Conv2D, AveragePooling1D, MaxPool1D, add, Flatten, Dense, Concatenate, Activation, Input\n",
    "from keras import regularizers, Model\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "tf.random.set_seed(2345)\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    x is a simple image, not a batch\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=5)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def folds(x, y, folds, num, start_index, end_index):\n",
    "\n",
    "    part = int(num/folds)\n",
    "\n",
    "    # 分割数据集;data1,data2分别为数据和标签\n",
    "    x_test = x[start_index * part:end_index * part]  # 数据刚好可以做k折交叉验证。\n",
    "    if start_index == 0:\n",
    "        x_train = x[end_index * part:]\n",
    "    else:\n",
    "        x_train0 = x[0:start_index * part]\n",
    "        x_train1 = x[end_index * part:]\n",
    "        x_train = np.concatenate((x_train0, x_train1), axis=0)\n",
    "\n",
    "    # 分割标签\n",
    "    y_test = y[start_index * part:end_index * part]  # 数据刚好可以做k折交叉验证。\n",
    "    if start_index == 0:\n",
    "        y_train = y[end_index * part:]\n",
    "    else:\n",
    "        y_train0 = y[0:start_index * part]\n",
    "        y_train1 = y[end_index * part:]\n",
    "        y_train = np.concatenate((y_train0, y_train1), axis=0)\n",
    "\n",
    "    print('fold:', folds, 'test:', start_index, ' train: other')\n",
    "\n",
    "    return x_train, x_test, y_train, y_test  # 每一折的训练和测试\n",
    "\n",
    "\n",
    "\n",
    "class mylstm(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(mylstm, self).__init__()\n",
    "\n",
    "        self.lstm = Sequential()\n",
    "        self.lstm.add(Bidirectional(LSTM(64, dropout=0.1, return_sequences=True)))\n",
    "        self.lstm.add(Bidirectional(LSTM(32, dropout=0.1, return_sequences=True)))\n",
    "\n",
    "\n",
    "        self.outlayer = Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            layers.Dense(5, activation='softmax')\n",
    "            ])\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "\n",
    "        x = self.lstm(inputs)\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = self.outlayer(x, training)\n",
    "\n",
    "        return x\n",
    "\n",
    "#导入数据\n",
    "data = np.load('/kaggle/working/Data.npy')\n",
    "label = np.load('/kaggle/working/Label.npy')\n",
    "\n",
    "data = tf.expand_dims(data, axis=2)\n",
    "db_num = int(data.shape[0])\n",
    "\n",
    "idx = tf.range(db_num)\n",
    "idx = tf.random.shuffle(idx)\n",
    "x, y = tf.gather(data, idx[:]), tf.gather(label, idx[:])\n",
    "\n",
    "fold = 5  # 5折交叉验证\n",
    "batchsz = 128\n",
    "# acc_avarage = []\n",
    "# loss_avarage = []\n",
    "\n",
    "for i in range(0, fold):\n",
    "    x_train, x_test, y_train, y_test = folds(x, y, fold, db_num, i, i+1)\n",
    "\n",
    "    db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    db_train = db_train.map(preprocess).shuffle(90000).batch(batchsz)\n",
    "\n",
    "    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    db_test = db_test.map(preprocess).batch(batchsz)\n",
    "\n",
    "    model = mylstm()\n",
    "    model.build(input_shape=(None,324,1))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    model.fit(db_train, epochs=30, validation_data=db_test, validation_freq=1)\n",
    "    loss, accuracy = model.evaluate(db_test)\n",
    "    \n",
    "    # 一次分类的训练\n",
    "#     if i == 0:\n",
    "#         break\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9728.35321,
   "end_time": "2022-02-11T15:45:54.082218",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-11T13:03:45.729008",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

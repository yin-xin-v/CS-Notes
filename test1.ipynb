{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536daebe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-28T11:41:50.581103Z",
     "iopub.status.busy": "2022-01-28T11:41:50.580316Z",
     "iopub.status.idle": "2022-01-28T11:41:50.630730Z",
     "shell.execute_reply": "2022-01-28T11:41:50.630209Z",
     "shell.execute_reply.started": "2022-01-28T10:45:57.006445Z"
    },
    "papermill": {
     "duration": 0.072549,
     "end_time": "2022-01-28T11:41:50.630903",
     "exception": false,
     "start_time": "2022-01-28T11:41:50.558354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mitbih1/117.atr\n",
      "/kaggle/input/mitbih1/103.dat\n",
      "/kaggle/input/mitbih1/106.hea\n",
      "/kaggle/input/mitbih1/113.dat\n",
      "/kaggle/input/mitbih1/202.dat\n",
      "/kaggle/input/mitbih1/217.dat\n",
      "/kaggle/input/mitbih1/111.atr\n",
      "/kaggle/input/mitbih1/208.hea\n",
      "/kaggle/input/mitbih1/219.dat\n",
      "/kaggle/input/mitbih1/104.atr\n",
      "/kaggle/input/mitbih1/201.dat\n",
      "/kaggle/input/mitbih1/201.atr\n",
      "/kaggle/input/mitbih1/121.hea\n",
      "/kaggle/input/mitbih1/219.atr\n",
      "/kaggle/input/mitbih1/102.dat\n",
      "/kaggle/input/mitbih1/116.hea\n",
      "/kaggle/input/mitbih1/231.dat\n",
      "/kaggle/input/mitbih1/111.hea\n",
      "/kaggle/input/mitbih1/209.hea\n",
      "/kaggle/input/mitbih1/122.atr\n",
      "/kaggle/input/mitbih1/203.hea\n",
      "/kaggle/input/mitbih1/212.atr\n",
      "/kaggle/input/mitbih1/232.dat\n",
      "/kaggle/input/mitbih1/100.hea\n",
      "/kaggle/input/mitbih1/234.hea\n",
      "/kaggle/input/mitbih1/214.dat\n",
      "/kaggle/input/mitbih1/119.hea\n",
      "/kaggle/input/mitbih1/228.dat\n",
      "/kaggle/input/mitbih1/107.dat\n",
      "/kaggle/input/mitbih1/108.hea\n",
      "/kaggle/input/mitbih1/123.hea\n",
      "/kaggle/input/mitbih1/121.dat\n",
      "/kaggle/input/mitbih1/105.atr\n",
      "/kaggle/input/mitbih1/232.hea\n",
      "/kaggle/input/mitbih1/233.hea\n",
      "/kaggle/input/mitbih1/115.dat\n",
      "/kaggle/input/mitbih1/212.hea\n",
      "/kaggle/input/mitbih1/200.atr\n",
      "/kaggle/input/mitbih1/214.atr\n",
      "/kaggle/input/mitbih1/213.hea\n",
      "/kaggle/input/mitbih1/223.dat\n",
      "/kaggle/input/mitbih1/122.dat\n",
      "/kaggle/input/mitbih1/221.atr\n",
      "/kaggle/input/mitbih1/100.dat\n",
      "/kaggle/input/mitbih1/203.dat\n",
      "/kaggle/input/mitbih1/111.dat\n",
      "/kaggle/input/mitbih1/222.hea\n",
      "/kaggle/input/mitbih1/203.atr\n",
      "/kaggle/input/mitbih1/230.atr\n",
      "/kaggle/input/mitbih1/123.dat\n",
      "/kaggle/input/mitbih1/223.atr\n",
      "/kaggle/input/mitbih1/215.atr\n",
      "/kaggle/input/mitbih1/233.atr\n",
      "/kaggle/input/mitbih1/205.hea\n",
      "/kaggle/input/mitbih1/215.dat\n",
      "/kaggle/input/mitbih1/201.hea\n",
      "/kaggle/input/mitbih1/213.dat\n",
      "/kaggle/input/mitbih1/220.dat\n",
      "/kaggle/input/mitbih1/115.atr\n",
      "/kaggle/input/mitbih1/117.dat\n",
      "/kaggle/input/mitbih1/219.hea\n",
      "/kaggle/input/mitbih1/116.dat\n",
      "/kaggle/input/mitbih1/113.hea\n",
      "/kaggle/input/mitbih1/118.atr\n",
      "/kaggle/input/mitbih1/109.hea\n",
      "/kaggle/input/mitbih1/202.atr\n",
      "/kaggle/input/mitbih1/231.hea\n",
      "/kaggle/input/mitbih1/234.dat\n",
      "/kaggle/input/mitbih1/109.atr\n",
      "/kaggle/input/mitbih1/124.hea\n",
      "/kaggle/input/mitbih1/106.dat\n",
      "/kaggle/input/mitbih1/212.dat\n",
      "/kaggle/input/mitbih1/105.dat\n",
      "/kaggle/input/mitbih1/210.atr\n",
      "/kaggle/input/mitbih1/101.hea\n",
      "/kaggle/input/mitbih1/200.hea\n",
      "/kaggle/input/mitbih1/223.hea\n",
      "/kaggle/input/mitbih1/107.atr\n",
      "/kaggle/input/mitbih1/123.atr\n",
      "/kaggle/input/mitbih1/214.hea\n",
      "/kaggle/input/mitbih1/231.atr\n",
      "/kaggle/input/mitbih1/107.hea\n",
      "/kaggle/input/mitbih1/234.atr\n",
      "/kaggle/input/mitbih1/215.hea\n",
      "/kaggle/input/mitbih1/112.atr\n",
      "/kaggle/input/mitbih1/220.hea\n",
      "/kaggle/input/mitbih1/105.hea\n",
      "/kaggle/input/mitbih1/108.dat\n",
      "/kaggle/input/mitbih1/221.dat\n",
      "/kaggle/input/mitbih1/210.dat\n",
      "/kaggle/input/mitbih1/210.hea\n",
      "/kaggle/input/mitbih1/102.atr\n",
      "/kaggle/input/mitbih1/228.atr\n",
      "/kaggle/input/mitbih1/113.atr\n",
      "/kaggle/input/mitbih1/207.atr\n",
      "/kaggle/input/mitbih1/115.hea\n",
      "/kaggle/input/mitbih1/117.hea\n",
      "/kaggle/input/mitbih1/222.dat\n",
      "/kaggle/input/mitbih1/103.atr\n",
      "/kaggle/input/mitbih1/112.dat\n",
      "/kaggle/input/mitbih1/118.hea\n",
      "/kaggle/input/mitbih1/109.dat\n",
      "/kaggle/input/mitbih1/200.dat\n",
      "/kaggle/input/mitbih1/217.atr\n",
      "/kaggle/input/mitbih1/207.hea\n",
      "/kaggle/input/mitbih1/114.atr\n",
      "/kaggle/input/mitbih1/205.atr\n",
      "/kaggle/input/mitbih1/104.hea\n",
      "/kaggle/input/mitbih1/228.hea\n",
      "/kaggle/input/mitbih1/124.atr\n",
      "/kaggle/input/mitbih1/208.atr\n",
      "/kaggle/input/mitbih1/213.atr\n",
      "/kaggle/input/mitbih1/119.atr\n",
      "/kaggle/input/mitbih1/119.dat\n",
      "/kaggle/input/mitbih1/232.atr\n",
      "/kaggle/input/mitbih1/217.hea\n",
      "/kaggle/input/mitbih1/103.hea\n",
      "/kaggle/input/mitbih1/106.atr\n",
      "/kaggle/input/mitbih1/208.dat\n",
      "/kaggle/input/mitbih1/205.dat\n",
      "/kaggle/input/mitbih1/207.dat\n",
      "/kaggle/input/mitbih1/114.dat\n",
      "/kaggle/input/mitbih1/220.atr\n",
      "/kaggle/input/mitbih1/124.dat\n",
      "/kaggle/input/mitbih1/209.dat\n",
      "/kaggle/input/mitbih1/114.hea\n",
      "/kaggle/input/mitbih1/100.atr\n",
      "/kaggle/input/mitbih1/108.atr\n",
      "/kaggle/input/mitbih1/230.dat\n",
      "/kaggle/input/mitbih1/121.atr\n",
      "/kaggle/input/mitbih1/221.hea\n",
      "/kaggle/input/mitbih1/122.hea\n",
      "/kaggle/input/mitbih1/101.atr\n",
      "/kaggle/input/mitbih1/233.dat\n",
      "/kaggle/input/mitbih1/file_list.txt\n",
      "/kaggle/input/mitbih1/104.dat\n",
      "/kaggle/input/mitbih1/102.hea\n",
      "/kaggle/input/mitbih1/222.atr\n",
      "/kaggle/input/mitbih1/209.atr\n",
      "/kaggle/input/mitbih1/118.dat\n",
      "/kaggle/input/mitbih1/202.hea\n",
      "/kaggle/input/mitbih1/112.hea\n",
      "/kaggle/input/mitbih1/101.dat\n",
      "/kaggle/input/mitbih1/116.atr\n",
      "/kaggle/input/mitbih1/230.hea\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a601ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T11:41:50.663690Z",
     "iopub.status.busy": "2022-01-28T11:41:50.662885Z",
     "iopub.status.idle": "2022-01-28T11:42:05.988322Z",
     "shell.execute_reply": "2022-01-28T11:42:05.987324Z",
     "shell.execute_reply.started": "2022-01-28T10:45:57.076635Z"
    },
    "papermill": {
     "duration": 15.351022,
     "end_time": "2022-01-28T11:42:05.988478",
     "exception": false,
     "start_time": "2022-01-28T11:41:50.637456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample\n",
    "import glob\n",
    "import pywt\n",
    "\n",
    "#--------------------小波去噪-----------------\n",
    "def WTfilt_1d(sig):\n",
    "    \"\"\"\n",
    "    对信号进行小波变换滤波\n",
    "    :param sig: 输入信号，1-d array\n",
    "    :return: 小波滤波后的信号，1-d array\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt    \n",
    "\n",
    "\n",
    "#-------------------------心拍截取-------------------\n",
    "def heartbeat(file0):\n",
    "    '''\n",
    "    file0:下载的MITAB数据\n",
    "    \n",
    "    '''\n",
    "    N_Seg=[]; SVEB_Seg=[];  VEB_Seg=[]; F_Seg=[] ; Q_Seg=[];\n",
    "    #--------去掉指定的四个导联的头文件---------\n",
    "    De_file=[panth[:-1]+'\\\\102.hea',panth[:-1]+'\\\\104.hea',panth[:-1]+'\\\\107.hea',panth[:-1]+'\\\\217.hea']\n",
    "    file=list(set(file0).difference(set(De_file)))\n",
    "    \n",
    "    for f in range(len(file)) :\n",
    "        annotation= wfdb.rdann(panth+file[f][-7:-4],'atr')\n",
    "        record_name=annotation.record_name    #读取记录名称\n",
    "        Record=wfdb.rdsamp(panth+record_name)[0][:,0] #一般只取一个导联\n",
    "        record=WTfilt_1d(Record)         #小波去噪\n",
    "        label=annotation.symbol  #心拍标签列表\n",
    "        label_index=annotation.sample   #标签索引列表\n",
    "        for j in range(len(label_index)):\n",
    "            if label_index[j]>=144  and (label_index[j]+180)<=650000:\n",
    "                if label[j]=='N' or label[j]=='.' or label[j]=='L' or label[j]=='R' or label[j]=='e' or label[j]=='j':\n",
    "                    Seg=record[label_index[j]-144:label_index[j]+180]#R峰的前0.4s和后0.5s\n",
    "                    segment=resample(Seg,251, axis=0)  #重采样到251\n",
    "                    N_Seg.append(segment)\n",
    "                    \n",
    "                if label[j]=='A' or label[j]=='a' or label[j]=='J' or label[j]=='S':\n",
    "                    \n",
    "                    Seg=record[label_index[j]-144:label_index[j]+180]\n",
    "                    segment=resample(Seg,251, axis=0) \n",
    "                    SVEB_Seg.append(segment)\n",
    "                    \n",
    "                if label[j]=='V' or label[j]=='E':\n",
    "                   \n",
    "                    Seg=record[label_index[j]-144:label_index[j]+180]\n",
    "                    segment=resample(Seg,251, axis=0)  \n",
    "                    VEB_Seg.append(segment)\n",
    "                    \n",
    "                if label[j]=='F':\n",
    "                    \n",
    "                    Seg=record[label_index[j]-144:label_index[j+1]+180]\n",
    "                    segment=resample(Seg,251, axis=0)  \n",
    "                    F_Seg.append(segment)\n",
    "                if  label[j]=='/' or label[j]=='f' or label[j]=='Q':\n",
    "                    \n",
    "                    Seg=record[label_index[j]-144:label_index[j]+180]\n",
    "                    segment=resample(Seg,251, axis=0)  \n",
    "                    Q_Seg.append(segment)\n",
    "                    \n",
    "    N_segement=np.array(N_Seg)\n",
    "    SVEB_segement=np.array(SVEB_Seg)\n",
    "    VEB_segement=np.array(VEB_Seg)\n",
    "    F_segement=np.array(F_Seg)\n",
    "    Q_segement=np.array(Q_Seg)\n",
    "    \n",
    "    label_N=np.zeros(N_segement.shape[0])\n",
    "    label_SVEB=np.ones(SVEB_segement.shape[0])\n",
    "    label_VEB=np.ones(VEB_segement.shape[0])*2\n",
    "    label_F=np.ones(F_segement.shape[0])*3\n",
    "    label_Q=np.ones(Q_segement.shape[0])*4\n",
    "                    \n",
    "    Data=np.concatenate((N_segement,SVEB_segement,VEB_segement,F_segement),axis=0)\n",
    "    Label=np.concatenate((label_N,label_SVEB,label_VEB,label_F,),axis=0)\n",
    "    \n",
    "    return  Data, Label\n",
    "\n",
    "#-----------------------心拍截取和保存---------------------\n",
    "#建议一次性截取和保存，不需要重复操作，下次训练和测试的时候，直接load\n",
    "panth='/kaggle/input/mitbih1/'\n",
    "file = glob.glob(panth+'*.hea')\n",
    "Data, Label=heartbeat(file)\n",
    "\n",
    "Data=np.save('/kaggle/working/'+'Data',Data)\n",
    "Label=np.save('/kaggle/working/'+'Label',Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52afdc51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T11:42:06.013585Z",
     "iopub.status.busy": "2022-01-28T11:42:06.012957Z",
     "iopub.status.idle": "2022-01-28T11:42:56.045835Z",
     "shell.execute_reply": "2022-01-28T11:42:56.046561Z",
     "shell.execute_reply.started": "2022-01-28T11:38:09.122847Z"
    },
    "papermill": {
     "duration": 50.053265,
     "end_time": "2022-01-28T11:42:56.046734",
     "exception": false,
     "start_time": "2022-01-28T11:42:05.993469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 248, 8)            32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 124, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 119, 16)           768       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 59, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 944)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               241920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 251,076\n",
      "Trainable params: 251,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1268/1268 [==============================] - 13s 4ms/step - loss: 0.0914 - categorical_accuracy: 0.9786\n",
      "Epoch 2/3\n",
      "1268/1268 [==============================] - 8s 5ms/step - loss: 0.0502 - categorical_accuracy: 0.9884 - val_loss: 0.0525 - val_categorical_accuracy: 0.9889\n",
      "Epoch 3/3\n",
      "1268/1268 [==============================] - 7s 3ms/step - loss: 0.0405 - categorical_accuracy: 0.9907\n",
      "317/317 [==============================] - 1s 2ms/step - loss: 0.0504 - categorical_accuracy: 0.9877\n",
      "saved total model.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "from tensorflow.keras.layers import MaxPooling1D, Conv1D, Conv2D, AveragePooling1D, MaxPool1D, add, Flatten, Dense, Concatenate, Activation\n",
    "from keras import regularizers\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "tf.random.set_seed(2345)\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    x is a simple image, not a batch\n",
    "    \"\"\"\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=4)\n",
    "    return x, y\n",
    "\n",
    "#导入数据\n",
    "data = np.load('/kaggle/working/Data.npy')\n",
    "label = np.load('/kaggle/working/Label.npy')\n",
    "\n",
    "data = tf.expand_dims(data, axis=2)\n",
    "db_num = int(data.shape[0])\n",
    "train_num = int(data.shape[0]/5*4)\n",
    "idx = tf.range(db_num)\n",
    "idx = tf.random.shuffle(idx)\n",
    "x_train, y_train = tf.gather(data, idx[:train_num]), tf.gather(label, idx[:train_num])\n",
    "x_test, y_test = tf.gather(data, idx[train_num:]) , tf.gather(label, idx[train_num:])\n",
    "\n",
    "\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "db_train = db_train.map(preprocess).shuffle(90000).batch(64)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "db_test = db_test.map(preprocess).shuffle(30000).batch(64)\n",
    "\n",
    "sample = next(iter(db_train))\n",
    "\n",
    "\n",
    "#cv+p+cv+p\n",
    "model_layers = [\n",
    "    # unit 1\n",
    "    Conv1D(filters=8,kernel_size=4,kernel_initializer=\"he_uniform\",strides=1,padding='valid',\n",
    "                   use_bias=False,kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Conv1D(filters=16,kernel_size=6,kernel_initializer=\"he_uniform\",strides=1,padding='valid',\n",
    "                   use_bias=False,kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    MaxPool1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    # fc*3\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = Sequential(model_layers)\n",
    "\n",
    "    model.build(input_shape=(None, 251, 1))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['categorical_accuracy']\n",
    "                  )\n",
    "    model.fit(db_train, epochs=3, validation_data=db_test, validation_freq=2)\n",
    "    model.evaluate(db_test)\n",
    "    \n",
    "    \n",
    "    model.save('./model.h5')\n",
    "    print('saved total model.')\n",
    "    \n",
    "    del model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623b071e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T11:42:56.210117Z",
     "iopub.status.busy": "2022-01-28T11:42:56.209333Z",
     "iopub.status.idle": "2022-01-28T11:42:57.825278Z",
     "shell.execute_reply": "2022-01-28T11:42:57.824851Z",
     "shell.execute_reply.started": "2022-01-28T11:39:47.190152Z"
    },
    "papermill": {
     "duration": 1.70333,
     "end_time": "2022-01-28T11:42:57.825404",
     "exception": false,
     "start_time": "2022-01-28T11:42:56.122074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from file.\n",
      "317/317 [==============================] - 2s 2ms/step - loss: 0.0504 - categorical_accuracy: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0504428930580616, 0.9876731634140015]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loaded model from file.')\n",
    "model = tf.keras.models.load_model('./model.h5', compile=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['categorical_accuracy']\n",
    "                      )\n",
    "\n",
    "model.evaluate(db_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 78.304839,
   "end_time": "2022-01-28T11:43:00.745165",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-28T11:41:42.440326",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
